### 课程预习：

在浏览本课程之前，可以先问自己以下问题，来测试你对这些问题的熟悉程度，判断你是否需要进一步观看课程。

#### Part1: 深度学习实践需要考虑的问题

1. 训练集、测试集、验证集

你能说出这三个概念的区别吗？
你知道常用的划分比例是什么？
当数据量很大时，划分比例会有变化吗？如果有应该是多少？
对于三个数据集的数据分布，我们通常会做什么要求才能尽量保证模型的泛化能力？

2. 偏差与方差

对于模型而言，什么是高偏差问题，什么是高方差问题？他们分别有什么表现？
当你运行一个模型后，得出训练误差和验证集误差，如何根据这两个误差判断模型的偏差、方差问题？
你能想出几种方法解决模型高偏差问题？
你能想出几种方法解决模型高方差问题？
深度学习方法中，还存在bias-variance tradeoff的问题吗

3. 正则化方法

你能说出几种在神经网络中常用的正则化方法？
L2罚为什么可以防止过拟合？为什么又叫做weight decay?
drop-out为什么可以防止过拟合？使用它还能观察loss function吗？
data augmentation跟真正的增加数据有什么区别
early stopping 是怎么工作的？实践中使用它有什么缺点？

4. 标准化

需要对输入标准化吗？标准化的好处有哪些？

5. 初值

在深度神经网络的训练中，权重初值经常会遭遇什么问题？
怎样设置权重的初值能够一定程度上防止权重的这些问题?

6. Gradient checking

如何利用gradient checking来检查code是否正确？
code正确是，gradient checking的结果有什么标志？


#### Part2: 常用的优化算法

1. Gradient descent

batch gradient descent/mini-batch gradient descent 有什么区别？具体的操作过程是什么
stochastic gradient descent是什么？为什么实操中不经常用它？
选择mini-batch的经验样本量大概是多少，如何选择你的size?

2. Exponentially weighted average

Exponentially weighted average 的方法是怎么计算的？它相当于把前几个阶段的步数做平均？
为什么要对它做bias correction? 怎么做呢？

3. Gradient decent momatal 

Gradient decent momatal 是怎么计算的？
它主要有那两方面的好处？
beta一般的选择默认值是多少？

4. RMSprop

RMSprop 是怎么计算的？
它主要有那两方面的好处？
beta,epslon一般的选择默认值是多少？

5. Adam

Adam 是怎么计算的？
这里在计算的时候是否需要bias correction?
beta1,beta2,epslon一般的选择默认值是多少？

6. Learning rate decay

为什么需要不断降低学习率
如何降低学习率？

7. Local minimal /saddle point

什么是loacl minimal?和saddle points
当遇到plateaus的时候，学习率会有什么表现？


#### Part4: 如何系统的调节参数

1. 神经网络的三类参数以及优先调试顺序

2. 选择参数点的技巧：随机、领域选择、缩放scale, 方式（babysit or parallel)

3. batch norm的技巧

batch norm 是怎么运行的?
Covariate shift是什么?
batch norm会加快运行速度的两个方面原因。
为什么说batch norm 还有正则化的效果
在测试的时候需要注意什么？

4. softmax layer

它的工作原理是什么？
loss function 是什么？

5. tensorflow 的使用示例




